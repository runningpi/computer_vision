{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 3 (Network)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 25<br>\n",
    "**Due:** 16.11.2022, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cacdc3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Loss](#1-Loss)\n",
    "- [2 Optimization](#2-Optimization-(5-Points))\n",
    "  - [2.1 Gradient Descent with Momentum](#2.1-Gradient-Descent-with-Momentum-(3-Points))\n",
    "  - [2.2 Weight Decay](#2.2-Weight-Decay-(2-Points))\n",
    "- [3 Deep Neural Network](#3-Deep-Neural-Network-(20-Points))\n",
    "  - [3.1 Definition](#3.1-Definition-(5-Points))\n",
    "  - [3.2 Training](#3.2-Training-(15-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51799a38",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "Besides the NumPy and Matplotlib libraries, we import the definitions of the network layers and the corresponding test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b791b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from modules import *\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d690495",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "### 1 Loss\n",
    "\n",
    "---\n",
    "\n",
    "In this assignment we want to create and train a deep neural network for classification, hence we need a loss function. We want to use again the cross-entropy loss that we already implemented in the last assignment.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.1 Task\n",
    "\n",
    "Complete the definition of the `CrossEntropyLoss` class in the `modules/loss.py` file.\n",
    "\n",
    "Feel free to use the implementation shown in the solution for last week's assignment, but you can also copy your own solution. This exercise is not graded. If you have not implemented this loss in the previous assignment, we highly recommend that you try this yourself before using the given solution.\n",
    "\n",
    "In the `forward` method of the class, compute the cross-entropy loss and store the result in the `out` variable that is returned from the method. Also cache the labels and the softmax probabilities to reuse them in the backward pass for gradient computation.\n",
    "\n",
    "In the `backward` method compute the gradient of the loss with respect to the inputs. Store the gradient in the `in_grad` variable that is passed to the given model and that is returned from the method.\n",
    "\n",
    "Use only vectorized operations.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.2 Test\n",
    "\n",
    "To test the implementation you can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5833de",
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropyLoss_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0e277",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 Optimization (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "So far we have only used vanilla gradient descent. That is, the update rule was to scale the gradient with the learning rate and subtract the result from the parameters. In this exercise we want to extend that concept a bit, taking into account also the previous updates.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.1 Gradient Descent with Momentum (3 Points)\n",
    "\n",
    "---\n",
    "\n",
    "One of the problems with standard gradient descent is that the gradient with respect to a parameter may change rapidly during training. These oscillations of the gradient make optimization hard. In addition, there is also the problem of the gradient being stuck in a flat region, where the slope is almost zero. Gradient descent with momentum is one approach to tackle these problems.\n",
    "\n",
    "Instead of just updating the parameters with\n",
    "\n",
    "$$\n",
    "    W^{(t)} = W^{(t-1)} - \\eta\\nabla\\mathcal{L}\\left(W^{(t-1)}\\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\eta$ is the learning rate, we also take into account the previous updates of the parameters, scaled by a hyperparameter called momentum. Thus the update rule becomes\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    W^{(t)} = W^{(t-1)} - V^{(t)}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "    V^{(t)} = \\mu V^{(t-1)} + \\eta\\nabla\\mathcal{L}\\left(W^{(t-1)}\\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $V$ is called velocity and $\\mu$ is the momentum. The velocity is an array with the same shape as $W$ and $\\nabla\\mathcal{L}(W)$ and can be understood as a moving average over the past gradients. With this update rule we get a more stable trajectory towards a minimum and we can still move, even if the gradient for the current time step becomes small.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1.1 Task\n",
    "\n",
    "Complete the definition of the `SGD` class in the `modules/optim.py` file.\n",
    "\n",
    "In the `step` method of the optimizer, implement the update rule described above. The $\\text{learning_rate}$ and $\\text{momentum}$ are stored as attributes of the optimizer object and each layer that is iterated over has dictionaries for parameters, gradients and velocity, where corresponding entries are referenced with the same name, given that you adhered to this convention in the previous exercises.\n",
    "\n",
    "Your implementation should be fully vectorized, so no loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1.2 Test\n",
    "\n",
    "To test your implementation, run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa929f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c17f6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 Weight Decay (2 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In the last assignment, we used L2 regularization to regularize our linear classifier models, computing the squared Euclidean norm of the parameters. We added explicitly a regularization loss term to the data loss term to compute the final loss. However, we can also implement this in a slightly different way.\n",
    "\n",
    "Instead of computing an explicit loss, we can apply **weight decay**, by just adding the gradient of the L2 regularization loss separately for each parameter to the gradient of the data loss. Hence, for vanilla gradient descent with weight decay we compute\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    W^{(t)} = W^{(t-1)} - \\eta\\left(\\nabla\\mathcal{L}\\left(W^{(t-1)}\\right) + \\lambda W^{(t-1)}\\right),\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\eta$ is the learning rate and $\\lambda$ is the regularization strength. This is based on the equivalent definition of the regularization loss as\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    R(W) = \\frac{\\lambda}{2}\\lVert W \\rVert^2.\n",
    "$$\n",
    "\n",
    "such that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial W} R(W) = \\lambda W.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "In the same way we can compute the update when using stochastic gradient descent with momentum. In this case we compute\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    W^{(t)} = W^{(t-1)} - V^{(t)}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "    V^{(t)} = \\mu V^{(t-1)} + \\eta\\left(\\nabla\\mathcal{L}\\left(W^{(t-1)}\\right) + \\lambda W^{(t-1)}\\right).\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.2.1 Task\n",
    "\n",
    "Extend the definition of the `SGD` class in the `modules/optim.py` file.\n",
    "\n",
    "Add weight decay to the update rule of stochastic gradient descent with momentum, which you implemented in the previous exercise.\n",
    "\n",
    "Use only vectorized operations.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.2.2 Test\n",
    "\n",
    "To test your implementation, run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_test(use_weight_decay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19b99b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3 Deep Neural Network (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have all the components implemented, we can plug everything together to create a deep neural network.\n",
    "\n",
    "Since we don't have GPU support, we're going to create a rather shallow model. Otherwise the training would take too much time. In order to get an idea how many parameters our model will have in the end, we'll calculate them from hand after the definition.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3.1 Definition (5 Points)\n",
    "\n",
    "To test our implementations, we're going to define a network with two convolutional layers, each followed by a ReLU activation function and a max pooling layer. We convert the outputs of the last pooling layer into vectors and pass them into a small fully-connected network, composed of two linear layers, the first of which has a ReLU activation function. The last linear layer has no activation function and produces the scores for the ten classes of the dataset.\n",
    "\n",
    "For both convolutional layers we use a kernel size of 3 and set padding and stride to 1. The first conv layer has 3 input channels and 6 output channels. The second conv layer has 6 input channels and 8 output channels.\n",
    "\n",
    "For the pooling layers we use a kernel size of 2 and a stride of the same size, so that we pool non-overlapping windows of the feature maps.\n",
    "\n",
    "The number of output features for the first linear layer should be 32, and for the second linear layer it should be 10, matching the number of classes in the CIFAR-10 dataset that we use again in this exercise.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.1.1 Feature Size (1 Point)\n",
    "\n",
    "The resolution of the images in the CIFAR-10 dataset is $32\\times32$. Given the definitions above, compute the number of input features for the first linear layer. Write down all the steps of your computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35bc47",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3f83f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.2 Implementation (2 Points)\n",
    "\n",
    "Complete the definition of the `ConvNet` class below.\n",
    "\n",
    "If you don't define the `forward` method, the inherited method from the base class will call the layers in the order in which they were added as attributes in the constructor. You don't have to define a `backward` method.\n",
    "\n",
    "Create the network according to the above definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Create deep neural network with two conv and two linear layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3527ee",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1.3 Capacity (2 Points)\n",
    "\n",
    "Now we want to compute the capacity of the model, which is the number of learnable parameters. Compute the number of parameters for each layer and than sum the results to get the total number of parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ab07d",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d58d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3.2 Training (15 Points)\n",
    "\n",
    "---\n",
    "\n",
    "We want to train our model again on the CIFAR-10 dataset that we already used in the previous problem sets. The function for loading and preprocessing the data expects the dataset in the `datasets` folder in the same directory as the notebook, so copy the folder before you proceed.\n",
    "\n",
    "Let's load the data and print the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the CIFAR-10 dataset.\n",
    "data = get_CIFAR_10_data()\n",
    "\n",
    "# Output the shapes of the partitioned data and labels.\n",
    "for name, array in data.items():\n",
    "    print(f'{name} shape: {array.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2f57b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.1 Task (12 Points)\n",
    "\n",
    "Implement a training loop for the defined model and dataset.\n",
    "\n",
    "In each iteration, randomly sample a minibatch of $64$ images from the development set with replacement. Compute the forward pass through the network. In order to do this, you can call the model directly. The `__call__` method dispatches to the `forward` method of the instance.\n",
    "\n",
    "Compute the average accuracy for the training batch and store it in the predefined `train_acc` list.\n",
    "\n",
    "The next step is to call the loss function with the model output and the ground truth labels. Again, you can call the instance directly. Store the loss in the `train_loss` list that is already defined. After that, call the `backward` method of the loss to compute the gradients of the loss with respect to the model parameters.\n",
    "\n",
    "In order to update the parameters, call the `step` method of the optimizer.\n",
    "\n",
    "Finally, sample a minibatch of the same size from the validation set and compute a forward pass. Again compute the loss. Store it in the `val_loss` list. Compute the average accuracy of the predictions and store the result in the `val_acc` list.\n",
    "\n",
    "Use only vectorized operations. No further loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "You're model should at least converge, so the loss should decrease and the accuracy increase. With the given settings, expect a slow start, but towards the end of the given number of iterations, you should see that the accuracy on the development set is well above chance, which would be $10\\%$.\n",
    "\n",
    "Since we're training only on the CPU, be prepared that training the model with the predefined settings may take a while!\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.2.2 Solution\n",
    "\n",
    "Write your solution in the marked code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "model = ConvNet()\n",
    "\n",
    "# Create the loss function.\n",
    "loss = CrossEntropyLoss(model)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = SGD(model, lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Access development set.\n",
    "X_dev = data['X_dev']\n",
    "y_dev = data['y_dev']\n",
    "\n",
    "# Access validation set.\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f268bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store training and validation loss.\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# Lists to store the training and validation accuracy.\n",
    "train_acc = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of iterations.\n",
    "num_iter = 100\n",
    "\n",
    "# Set number of samples per minibatch.\n",
    "batch_size = 64\n",
    "\n",
    "# Show intermediate results.\n",
    "verbose = True\n",
    "print_every = 10\n",
    "\n",
    "# Train the model.\n",
    "for i in range(1, 1+num_iter):\n",
    "    ############################################################\n",
    "    ###                  START OF YOUR CODE                  ###\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    ###                   END OF YOUR CODE                   ###\n",
    "    ############################################################\n",
    "    if verbose and (i == 1 or i % print_every == 0):\n",
    "        print(\n",
    "            f'Iter: {i:4}  | ',\n",
    "            f'Train acc: {train_acc[-1]*100:6.2f}%  | ',\n",
    "            f'Val acc: {val_acc[-1]*100:6.2f}%  | ',\n",
    "            f'Train loss: {train_loss[-1]:6.3f}  | ',\n",
    "            f'Val loss: {val_loss[-1]:6.3f}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e0973",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.3 Results\n",
    "\n",
    "Let's check the best accuracy on the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42281f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best train acc: {np.max(train_acc)*100:6.2f}  |  Best val acc: {np.max(val_acc)*100:6.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a39e83",
   "metadata": {},
   "source": [
    "Let's also plot the training and validation losses and accuracies obtained during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training(train_loss, val_loss, train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80991241",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2.4 Observations (3 Points)\n",
    "\n",
    "Briefly describe your observations when you trained the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d87b0e",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a8bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
