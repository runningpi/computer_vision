{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 3 (Layer)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 70<br>\n",
    "**Due:** 16.11.2022, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cacdc3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Vectorization Layer](#1-Vectorization-Layer)\n",
    "- [2 Linear Layer](#2-Linear-Layer)\n",
    "- [3 Convolutional Layer](#3-Convolutional-Layer)\n",
    "- [4 Max Pooling](#4-Max-Pooling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51799a38",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "Besides the NumPy and Matplotlib libraries, we import the definitions of the network layers and the corresponding test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b791b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from modules.layer import *\n",
    "from modules.layer_test import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c936c3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851329c7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1 Vectorization Layer (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "We want to be able to use linear and convolutional layers in the same network. Hence we need a function to convert tensor inputs into vectors and vice versa.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.1 Implementation\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `Vector` class in the `modules/layer.py` file.\n",
    "\n",
    "In the `forward` method, store the shape of the input to be used in the backward pass. Convert the inputs into vectors such that the result is a matrix where each row is one input. Store the result in the `out` variable that is returned from the method.\n",
    "\n",
    "In the `backward` method, apply the reversed operation and restore the original shape for the gradient of the loss that the method receives from the following layer. Store the result in the `in_grad` variable that is returned from the method.\n",
    "\n",
    "Your implementation should be fully vectorized, so no loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.1.1 Test\n",
    "\n",
    "To test your implementation, run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b72e5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 Linear Layer (15 Points)\n",
    "\n",
    "---\n",
    "\n",
    "A fully-connected neural network is composed of linear or affine layers that map input vectors with shape $(\\text{num_samples},\\text{in_features})$ to output vectors with shape $(\\text{num_samples},\\text{out_features})$, where the difference between a linear and an affine map is whether a bias is added or not. In this exercise we want to implement such a layer as described below.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.1 Implementation\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of `Linear` class in the `modules/layer.py` file.\n",
    "\n",
    "In the `__init__` method, store a flag indicating if a linear or affine transformation should be used.<br>\n",
    "Initialize the parameters with random values as follows:<br>\n",
    "\n",
    "- The parameters are stored in a dictionary already created in the base class. Save the weights and, conditionally, the bias, as in\n",
    "  ```python\n",
    "  self.param['weight'] = ...\n",
    "  self.param['bias'] = ...\n",
    "  ```\n",
    "- The weights should have the shape $(\\text{in_features},\\text{out_features})$ and the bias should have the shape $(\\text{out_features})$, where $\\text{in_features}$ is the length of the input vectors and $\\text{out_features}$ is the length of the output vectors.\n",
    "- Initialize each parameter value from $\\text{Uniform}(-\\sqrt{k},\\sqrt{k})$ where\n",
    "\n",
    "  $$\n",
    "      k = \\frac{1}{\\text{in_features}}.\n",
    "  $$\n",
    "   \n",
    "<br>\n",
    "\n",
    "In the `forward` method, store the received inputs for gradient computation in the backward pass. Depending on the stored flag, apply a linear or affine transformation to the inputs and store the result in the `out` variable that is returned from the method.\n",
    "   \n",
    "<br>\n",
    "\n",
    "In the `backward` method, compute the gradient of the loss with respect to the parameters and inputs. The method receives the gradient of the loss with respect to the layer output.\n",
    "   \n",
    "- The gradient of the loss with respect to the weights and, if required, the bias, is stored in a dictionary inherited from the base class. Save the parameters using the same keys that were used for the parameter dictionary, as in\n",
    "  ```python\n",
    "  self.grad['weight'] = ...\n",
    "  self.grad['bias'] = ...\n",
    "  ```\n",
    "- Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
    "\n",
    "Your implementation should be fully vectorized, that is, no loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1.1 Test\n",
    "\n",
    "To test your implementation, run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c67fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efda462",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3 Convolutional Layer (25 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In order to implement a CNN, we need convolutional layers. For inputs with shape $(\\text{num_samples},\\text{in_channels},\\text{in_height},\\text{in_width})$ the layer should generate output with shape $(\\text{num_samples},\\text{out_channels},\\text{out_height},\\text{out_width})$, with the spacial dimensions of the output depending on the size of the input, the kernel size and the padding and stride applied in the convolution.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3.1 Implementation\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of `Conv2d` class in the `modules/layer.py` file.\n",
    "\n",
    "In the `__init__` method, store the given values for padding and stride, and a flag indicating if a bias should be used or not.<br>\n",
    "Initialize the parameters with random values as follows:<br>\n",
    "\n",
    "- The parameters are stored in a dictionary already created in the base class. Save the weights and, conditionally, the bias, as in\n",
    "  ```python\n",
    "  self.param['weight'] = ...\n",
    "  self.param['bias'] = ...\n",
    "  ```\n",
    "- The weights should have the shape $(\\text{out_channels},\\text{in_channels},\\text{kernel_size},\\text{kernel_size})$ and the bias should have the shape $(\\text{out_channels})$, thus you can assume that the filters are always square.\n",
    "- Initialize each parameter value from $\\text{Uniform}(-\\sqrt{k},\\sqrt{k})$ where\n",
    "\n",
    "  $$\n",
    "      k = \\frac{1}{\\text{in_channels} \\:\\cdot\\: \\text{kernel_size}^2}.\n",
    "  $$\n",
    "   \n",
    "<br>\n",
    "\n",
    "In the `forward` method, store the received inputs for gradient computation in the backward pass. Convolve the inputs with the filters using the stored values for padding and stride and conditionally add the bias, then store the result in the `out` variable that is returned from the method.\n",
    "   \n",
    "<br>\n",
    "\n",
    "In the `backward` method, compute the gradient of the loss with respect to the parameters and inputs. The method receives the gradient of the loss with respect to the layer output.\n",
    "   \n",
    "- The gradient of the loss with respect to the weights and, if required, the bias, is stored in a dictionary inherited from the base class. Save the parameters using the same keys that were used for the parameter dictionary, as in\n",
    "  ```python\n",
    "  self.grad['weight'] = ...\n",
    "  self.grad['bias'] = ...\n",
    "  ```\n",
    "- Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
    "\n",
    "Your implementation should be at least partly vectorized, that is, you're allowed to use at most *four* loops each for the forward and backward pass.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3.1.1 Test\n",
    "\n",
    "To test your implementation, run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv2d_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64153d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4 Max Pooling (25 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In order to reduce the size of the feature maps in an CNN, we want to use max pooling. For each channel separately, the input is filtered such that only the maximum activation in each filter position is selected.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.1 Implementation\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of `MaxPool` class in the `modules/layer.py` file.\n",
    "\n",
    "In the `__init__` method, store the given values for kernel size and stride. If no stride is given, set the value to the kernel size, so that non-overlapping windows are used for pooling.\n",
    "   \n",
    "<br>\n",
    "\n",
    "In the `forward` method, store the received inputs for gradient computation in the backward pass. Apply max pooling to the input using the given kernel size and stride, then store the result in the `out` variable that is returned from the method.\n",
    "   \n",
    "<br>\n",
    "\n",
    "In the `backward` method, compute the gradient of the loss with respect to the inputs. The method receives the gradient of the loss with respect to the layer output. Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
    "\n",
    "Your implementation should be at least partly vectorized, that is, you're allowed to use at most *four* loops each for the forward and backward pass.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 4.1.1 Test\n",
    "\n",
    "To test your implementation, run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxPool_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ab1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
