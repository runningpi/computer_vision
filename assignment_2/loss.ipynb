{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 2 (Loss)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 60<br>\n",
    "**Due:** 10.11.2022, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64171c5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Multiclass SVM Loss](#1-Multiclass-SVM-Loss-(25-Points))\n",
    "  - [1.1 Implementation](#1.1-Implementation-(20-Points))\n",
    "  - [1.2 Margin Hyperparameter](#1.2-Margin-Hyperparameter-(5-Points))\n",
    "- [2 Cross-entropy Loss](#2-Cross-entropy-Loss-(30-Points))\n",
    "  - [2.1 Implementation](#2.1-Implementation-(20-Points))\n",
    "  - [2.2 Convexity](#2.2-Convexity-(10-Points))\n",
    "- [3 Square Loss](#3-Square-Loss-(5-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afd2c5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we use only the **Numpy** library.\n",
    "\n",
    "We import definitions of loss functions from the `loss.py` module and enable autoreload, so that the imported functions are automatically updated whenever the code is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3f4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from loss import SVM_loss, cross_entropy_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e972a8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "### 1 Multiclass SVM Loss (25 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In this exercise we want to implement the **multiclass SVM loss**, so that we can use it in conjunction with a linear classification model.\n",
    "\n",
    "Let's recap the definition of this loss with respect to a single training example $(\\mathbf{x}, y)$. Denoting the raw scores computed by the classifier with $\\mathbf{s} \\in \\mathbb{R}^K$ where $K$ is the number of categories, the loss is defined as\n",
    "\n",
    "$$\n",
    "    L(\\mathbf{s}) = \\sum_{k=1, k \\neq y}^K \\max(0, \\mathbf{s}_k - \\mathbf{s}_y + d),\n",
    "$$\n",
    "\n",
    "with $d$ being a hyperparameter representing the desired margin between the score for the correct class and the scores for the incorrect classes.\n",
    "\n",
    "The total loss for a dataset of $N$ examples, is just the average of the losses for a single input, that is for a score matrix $S \\in \\mathbb{R}^{N \\times K}$ we compute\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(S) = \\frac{1}{N} \\sum_{n=1}^N L(S_n)\n",
    "$$\n",
    "\n",
    "with row $S_n$ being the score vector obtained for the $n$-th example.\n",
    "\n",
    "The intuition with this objective function is that we don't really care about the absolute values or relative differences of the classifier's scores, as long as there's a sufficiently large margin between the prediction for the correct category and all others.\n",
    "\n",
    "Now, since we want to use gradient based algorithms to learn the parameters of our model, we have to compute the gradient of the loss with respect to the weights and biases. However, we can use the chain rule for derivatives to first compute the partial derivative of the loss with respect to the scores and then multiply the result with the partial derivative of the scores with respect to the model parameters.\n",
    "\n",
    "For a single example $(\\mathbf{x}, y)$, we compute the partial derivative of the loss with respect to the scores $\\mathbf{s}$ as\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\nabla L(\\mathbf{s})_k\n",
    "    =\n",
    "    \\frac{\\partial L}{\\partial \\mathbf{s}_k}\n",
    "    =\n",
    "    \\begin{cases}\n",
    "        -\\sum_{j=1,j \\neq y}^K \\mathbb{1}(\\mathbf{s}_j - \\mathbf{s}_y + d > 0)\n",
    "        &\n",
    "        \\text{if} \\enspace k = y\n",
    "        \\\\[0.5em]\n",
    "        \\hphantom{-\\sum_{j=1,j \\neq y}^K}\\,\\mathbb{1}(\\mathbf{s}_k - \\mathbf{s}_y + d > 0)\n",
    "        &\n",
    "        \\text{if} \\enspace k \\neq y\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\mathbb{1}$ is the indicator function, being $1$ if the condition is satisfied and $0$ otherwise.\n",
    "\n",
    "So, for the derivative of the loss with respect to the score corresponding to the correct class, we count the number of predictions contributing to the loss, while for the scores corresponding to the wrong classes, the derivative is just 1 if the respective score contributes to the loss and 0 otherwise.\n",
    "\n",
    "Regarding the partial derivative of the total loss $\\mathcal{L}$ with respect to the scores $S$, by linearity of the differential operator we obtain\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\nabla \\mathcal{L}(S) = \\frac{1}{N} \\sum_{n=1}^N \\nabla L(S_n).\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "506113cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5. ,  2.5,  0. , -2.5])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.arange(0,10, 0.5).reshape(4,5)\n",
    "y = np.arange(0,4)\n",
    "L = S-S[y]+1\n",
    "L = np.where(L<0, 0, L)\n",
    "np.fill_diagonal(L, 0)\n",
    "L = np.sum(L, axis=1)\n",
    "S = S- S[np.arange(len(S)), y].reshape(len(S), 1)\n",
    "np.sum(S, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310c332",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.1 Implementation (20 Points)\n",
    "\n",
    "Complete the definition of the `SVM_loss` function in the `loss.py` file.\n",
    "\n",
    "The function should take a matrix of scores $S$ with shape $(N, K)$ where $N$ is the number of samples and $K$ is the number of classes. Hence, $S_{n,k}$ is expected to be the score for the $k$-th class computed for the $n$-th input. The second parameter is a vector $\\mathbf{y}$ of labels with length $N$, so $\\mathbf{y}_n$ contains the correct label for the $n$-th input. The parameter for the margin $d$ is optional.\n",
    "\n",
    "The function should output a tuple $(L, \\mathrm{d}S)$ where $L$ is the total loss for the given input scores and $\\mathrm{d}S$ is the partial derivative of the loss $L$ with respect to $S$. So $L$ is a scalar and $\\mathrm{d}S$ should have the same shape as $S$.\n",
    "\n",
    "Use only vectorized NumPy operations for the implementation. No loops are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4354d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.1 Test\n",
    "\n",
    "To test your implementation you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "da3280c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0499999999999998"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dummy scores.\n",
    "S = np.array([\n",
    "    [1.2, 4.5, 5.0, 4.8],\n",
    "    [3.0, 1.4, 2.8, 0.5]\n",
    "])\n",
    "\n",
    "# Define labels.\n",
    "y = np.array([2, 0])\n",
    "\n",
    "# Compute loss and derivatives.\n",
    "L, dS = SVM_loss(S, y)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1e42270d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compare loss.\n",
    "loss_equal = abs(L - 1.05) < 1e-10\n",
    "\n",
    "# Compare derivatives.\n",
    "grad_equal = np.array_equal(dS, np.array([\n",
    "    [ 0.0, 0.5, -1.0, 0.5],\n",
    "    [-0.5, 0.0,  0.5, 0.0]\n",
    "]))\n",
    "\n",
    "# Show results.\n",
    "print(loss_equal and grad_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d666a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.2 Margin Hyperparameter (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Give a thorough explanation why we can set the hyperparameter $d$ of the multiclass SVM loss to a constant value, like $d = 1$, without conducting a hyperparameter search, if we use this data loss together with L2 regularization.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef557b",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eddac1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 Cross-entropy Loss (30 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Now we want to implement the **cross-entropy loss** so that we can use it with a linear classifier.\n",
    "\n",
    "Let's again first recap the definition. For a single training example $(\\mathbf{x}, y)$, the cross-entropy loss is defined as\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    L(\\mathbf{s})\n",
    "    =\n",
    "    -\\ln\\frac{e^{\\mathbf{s}_y}}{\\sum_{k=1}^K e^{\\mathbf{s}_k}}\n",
    "    =\n",
    "    -\\mathbf{s}_y + \\ln\\sum_{k=1}^K e^{\\mathbf{s}_k},\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\mathbf{s} \\in \\mathbb{R}^K$ is the unnormalized score of the linear classifier and $K$ is the number of classes.\n",
    "\n",
    "This derives from the definition of the cross-entropy between the distribution of the classifier's scores and the distribution represented by the one-hot encoded ground truth label for the respective sample, where all terms become zero except the one that corresponds to the true class. Therefore we only compute the loss for the $y$-th entry of $\\mathbf{s}$.\n",
    "\n",
    "The total loss for a dataset of $N$ examples is again just the average of the losses for a single input, that is for a score matrix $S \\in \\mathbb{R}^{N \\times K}$ we compute\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(S) = \\frac{1}{N} \\sum_{n=1}^N L(S_n)\n",
    "$$\n",
    "\n",
    "with row $S_n$ being the score vector obtained for the $n$-th example.\n",
    "\n",
    "In order to use gradient based optimization, we have to compute the partial derivatives of the cross-entropy loss with respect to the model parameters. However, we can again make use of the product rule for derivatives and first compute the partial derivatives with respect to the scores. For a single example, we can compute this as\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\nabla L(\\mathbf{s})_k\n",
    "    =\n",
    "    \\frac{\\partial L}{\\partial\\mathbf{s}_k}\n",
    "    =\n",
    "    \\begin{cases}\n",
    "        \\sigma(\\mathbf{s})_k - 1 & \\text{if} \\enspace k =    y \\\\[0.5em]\n",
    "        \\sigma(\\mathbf{s})_k     & \\text{if} \\enspace k \\neq y,\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\sigma$ denotes the softmax function, defined as\n",
    "\n",
    "$$\n",
    "    \\sigma(\\mathbf{s})_k = \\frac{e^{\\mathbf{s}_k}}{\\sum_{j=1}^K e^{\\mathbf{s}_j}}.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Regarding the gradient of the total loss $\\mathcal{L}$ with respect to the scores $S$, we can again use the linearity of the differential operator to obtain\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\nabla \\mathcal{L}(S) = \\frac{1}{N} \\sum_{n=1}^N \\nabla L(S_n).\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635393e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.1 Implementation (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `cross_entropy_loss` function in the `loss.py` file.\n",
    "\n",
    "The function should take a matrix of scores $S$ with shape $(N, K)$ where $N$ is the number of samples and $K$ is the number of classes. So $S_{n,k}$ is expected to be the score for the $k$-th class computed for the $n$-th input. The second parameter is a vector $y$ of labels with length $N$, so $y_n$ contains the correct label for the $n$-th input.\n",
    "\n",
    "The function should output a tuple $(L, \\mathrm{d}S)$ where $L$ is the total loss for the given input scores and $\\mathrm{d}S$ is the gradient of the loss with respect to $S$. So $L$ is a scalar and $\\mathrm{d}S$ should have the same shape as $S$.\n",
    "\n",
    "Use only vectorized NumPy operations for the implementation. No loops are allowed.\n",
    "\n",
    "<br>\n",
    "\n",
    "Please note that in order to avoid numerical problems when computing the exponential functions, you can use that\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\sigma(\\mathbf{s})_k\n",
    "    =\n",
    "    \\frac{e^{\\mathbf{s}_k}}{\\sum_{j=1}^K e^{\\mathbf{s}_j}}\n",
    "    =\n",
    "    \\frac{e^{-c}\\,e^{\\mathbf{s}_k}}{e^{-c}\\,\\sum_{j=1}^K e^{\\mathbf{s}_j}}\n",
    "    =\n",
    "    \\frac{e^{\\mathbf{s}_k-c}}{\\sum_{j=1}^K e^{\\mathbf{s}_j-c}}\n",
    "$$\n",
    "\n",
    "for any constant $c$.\n",
    "\n",
    "Since we don't know the actual values of the classifier's scores in advance, a sensible choice is to set\n",
    "\n",
    "$$\n",
    "    c = \\max\\{\\mathbf{s}_j\\}, \\enspace j = 1, \\ldots, K.\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2761074",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2.1.1 Test\n",
    "\n",
    "To test your implementation you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1608e381",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16588\\1548578764.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define dummy scores.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m S = np.array([\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Define dummy scores.\n",
    "S = np.array([\n",
    "    [1.2, 4.5, 5.0, 4.8],\n",
    "    [3.0, 1.4, 2.8, 0.5]\n",
    "])\n",
    "\n",
    "# Define labels.\n",
    "y = np.array([2, 0])\n",
    "\n",
    "# Compute loss and derivatives.\n",
    "L, dS = cross_entropy_loss(S, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e850f0bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16588\\3327634977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compare loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss_equal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.81917\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Compare derivatives.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m grad_equal = np.allclose(dS, np.array([\n",
      "\u001b[1;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare loss.\n",
    "loss_equal = abs(L - 0.81917) < 1e-5\n",
    "\n",
    "# Compare derivatives.\n",
    "grad_equal = np.allclose(dS, np.array([\n",
    "    [ 0.00456988, 0.12390151, -0.29572094, 0.16724955],\n",
    "    [-0.26221188, 0.04800859,  0.19468445, 0.01951884]\n",
    "]))\n",
    "\n",
    "# Show results.\n",
    "print(loss_equal and grad_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a5499",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 Convexity (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "An important condition that we want our loss functions to satisfy is that they are [convex functions](https://en.wikipedia.org/wiki/Convex_function), which implies that the functions have only one minimum.\n",
    "\n",
    "A function $f: X \\to \\mathbb{R}$ is convex, if for all $x_1, x_2 \\in X$ and $0 \\leq t \\leq 1$ holds that\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    f(tx_1 + (1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2).\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Show that the cross-entropy loss $\\mathcal{L}$ for N examples is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cb58f",
   "metadata": {},
   "source": [
    "##### Proof\n",
    "\n",
    "*Write your proof here.*\n",
    "\n",
    "<div style=\"text-align:right\">$\\square$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ab2db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3 Square Loss (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In *regression* tasks the **mean squared error** is often used as a loss function, which for $N$ examples can be defined as\n",
    "\n",
    "$$\n",
    "    L(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{N}\\sum_{n=1}^N (\\hat{\\mathbf{y}}_n - \\mathbf{y}_n)^2,\n",
    "$$\n",
    "\n",
    "with real predictions $\\hat{\\mathbf{y}}$ and target values $\\mathbf{y}$.\n",
    "\n",
    "However, from the above definition, it is not immediately clear how this loss could be applied for a *classification* task.\n",
    "\n",
    "Think about the problem and do some research. Provide a formula for the square loss that can be used as an objective function for classifiers and explain how it works, defining all components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c97143",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "This loss function requiers only one prediction value for a target value. In classification tasks, we mostly have one prediction value for each class. \n",
    "\n",
    "This problem could be solved by represent the class predictions in a continious function. Then we can define some thresholds for the classes. For instance, class 1 in between 0 \\leq \\hat y_n \\leq 0.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0182f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "98460849052666b5979b83e6345eea51d645249b9db211168c63d11c59230163"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
