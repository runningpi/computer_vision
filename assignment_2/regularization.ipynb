{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 2 (Regularization)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 10<br>\n",
    "**Due:** 10.11.2022, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:** Max Althaus\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64171c5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 L1 Regularization](#1-L1-Regularization-(5-Points))\n",
    "  - [1.1 Implementation](#1.1-Implementation-(3-Points))\n",
    "  - [1.2 Explanation](1.2-Explanation-(2-Points))\n",
    "- [2 L2 Regularization](#2-L2-Regularization-(5-Points))\n",
    "  - [2.1 Implementation](#2.1-Implementation-(3-Points))\n",
    "  - [2.2 Explanation](#2.2-Explanation-(2-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afd2c5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we use the only the **NumPy** library.\n",
    "\n",
    "We import definitions of regularizers from the `regularization.py` module and enable autoreload, so that the imported functions are automatically updated whenever the code is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3f4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from regularization import L1_reg, L2_reg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1f2db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbf55f",
   "metadata": {},
   "source": [
    "### 2 L1 Regularization (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In this exercise we want to implement **L1 regularization**. Here, the regularizer is the absolute value of the model's weights, defined as\n",
    "\n",
    "$$\n",
    "    R(W) = \\sum_{i=1}^D \\sum_{j=1}^K \\vert W_{i,j} \\vert.\n",
    "$$\n",
    "\n",
    "In order to control the effects of the regularization term, we introduce the regularization strength $\\lambda$ as a hyperparameter. The complete loss for our model is then the sum of the data loss $\\mathcal{L}$ and the regularization loss $R$, that is\n",
    "\n",
    "$$\n",
    "    J(W) = \\mathcal{L}(W) + \\lambda R(W).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6d12b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.1 Implementation (3 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `L1_reg` function in the `regularization.py` file.\n",
    "\n",
    "The function takes a parameter matrix $W$ of shape $(D+1, K)$, where $K$ is the number of categories and $D$ is the dimension of the inputs. The last row is assumed to be the bias. The second parameter is the regularization strength.\n",
    "\n",
    "The function should return a tuple $(R, dW)$ with the regularization loss $R$, computed only for the weights and not the bias, and the gradient of the loss $dW$ with respect to the parameters. So the loss $R$ is a scalar and $dW$ has the same shape as $W$.\n",
    "\n",
    "Use only vectorized NumPy operations for the implementation. No loops are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74b35d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Test 1.1.1\n",
    "\n",
    "To test your implementation, you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a5a86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dummy parameters.\n",
    "W = np.array([\n",
    "    [ 1.2,  3.6,  8.1],\n",
    "    [ 4.0, -1.0,  3.6],\n",
    "    [-9.6,  2.5, -6.3],\n",
    "    [ 3.5, -7.2, -2.0]\n",
    "])\n",
    "\n",
    "# Compute regularization loss.\n",
    "R, dW = L1_reg(W, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32c744",
   "metadata": {},
   "source": [
    "#### Wof√ºr bilden wir die Ableitung?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8254948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compare loss.\n",
    "loss_equal = abs(R - 19.95) < 1e-5\n",
    "\n",
    "# Compare derivatives.\n",
    "grad_equal = np.array_equal(dW, np.array([\n",
    "    [ 0.5,  0.5,  0.5],\n",
    "    [ 0.5, -0.5,  0.5],\n",
    "    [-0.5,  0.5, -0.5],\n",
    "    [ 0.0,  0.0,  0.0]\n",
    "]))\n",
    "\n",
    "# Show results.\n",
    "print(loss_equal and grad_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d399ca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.2 Explanation (2 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Briefly describe in your own words how the L1 regularization affects the parameters of the model.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af3e9c",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "The L1 regularization reduces the weights of the model to avoide overfitting. It reduces in a linear ratio to the sum of alle weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5845b93",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2 L2 Regularization (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In this exercise we want to implement **L2 regularization**. Here, the regularizer is the squared euclidean distance of the model's weights, defined as\n",
    "\n",
    "$$\n",
    "    R(W) = \\sum_{i=1}^D \\sum_{j=1}^K W_{i,j}^2.\n",
    "$$\n",
    "\n",
    "Again, we have the regularization strength $\\lambda$ as an additional hyperparameter, controlling by how much we restrict the model's parameters. The complete loss for our model is the sum of the data loss $\\mathcal{L}$ and the regularization loss $R$, that is\n",
    "\n",
    "$$\n",
    "    J(W) = \\mathcal{L}(W) + \\lambda R(W).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2e154",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.1 Implementation (3 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Complete the definition of the `L2_reg` function in the `regularization.py` file.\n",
    "\n",
    "The function takes a parameter matrix $W$ of shape $(D+1, K)$, where $K$ is the number of categories and $D$ is the dimension of the inputs. The last row is assumed to be the bias. The second parameter is the regularization strength.\n",
    "\n",
    "The function should return a tuple $(R, dW)$ with the regularization loss $R$, computed only for the weights and not the bias, and the gradient of the loss $dW$ with respect to the parameters. So the loss $R$ is a scalar and $dW$ has the same shape as $W$.\n",
    "\n",
    "Use only vectorized NumPy operations for the implementation. No loops are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec81b98",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2.1.1 Test\n",
    "\n",
    "To test your implementation, you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45271089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dummy parameters.\n",
    "W = np.array([\n",
    "    [ 1.2,  3.6,  8.1],\n",
    "    [ 4.0, -1.0,  3.6],\n",
    "    [-9.6,  2.5, -6.3],\n",
    "    [ 3.5, -7.2, -2.0]\n",
    "])\n",
    "\n",
    "# Compute regularization loss.\n",
    "R, dW = L2_reg(W, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458ba8a",
   "metadata": {},
   "source": [
    "#### Wieso wird hier die Ableitung ohne r gebildet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8572fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compare loss.\n",
    "loss_equal = abs(R - 124.035) < 1e-5\n",
    "\n",
    "# Compare gradient.\n",
    "grad_equal = np.array_equal(dW, [\n",
    "    [ 1.2,  3.6,  8.1],\n",
    "    [ 4.0, -1.0,  3.6],\n",
    "    [-9.6,  2.5, -6.3],\n",
    "    [ 0.0,  0.0,  0.0]\n",
    "])\n",
    "\n",
    "# Show results.\n",
    "print(loss_equal and grad_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f51de",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2 Explanation (2 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Briefly describe in your own words how the L2 regularization affects the parameters of the model.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be37d81",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "The L2 regularization reduces the weights in W in a quadratic way. So if there are very high values, it will penalize more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d7ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "98460849052666b5979b83e6345eea51d645249b9db211168c63d11c59230163"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
