{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfb5226",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Winter Semester 2022/23\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Assignment 2 (Classifier)*\n",
    "\n",
    "---\n",
    "\n",
    "**Points:** 40<br>\n",
    "**Due:** 10.11.2022, 10 am<br>\n",
    "**Contact:** Matthias Fulde ([fulde@cs.uni-frankfurt.de](mailto:fulde@cs.uni-frankfurt.de))<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Your Name:**\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Note\n",
    "\n",
    "You should first complete the implementations of the loss and regularization notebooks before starting with this notebook.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64171c5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [1 Linear Classifiers](#1-Linear-Classifiers-(40-Points))\n",
    "  - [1.1 Preprocessing](#1.1-Preprocessing-(5-Points))\n",
    "  - [1.2 Implementation](#1.2-Implementation-(20-Points))\n",
    "  - [1.3 Hyperparameter Search](#1.3-Hyperparameter-Search-(10-Points))\n",
    "  - [1.4 Limitations](#1.4-Limitations-(5-Points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afd2c5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we use the libraries **Matplotlib** and **NumPy**.\n",
    "\n",
    "We want Matplotlib figures to appear within the notebook rather than inside a separate window, which is default in some environments, therefore we make use of the `%matplotlib` magic function to set the Matplotlib backend to inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d83e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618170c",
   "metadata": {},
   "source": [
    "We import definitions of loss functions from the `loss.py` module, regularizers from the `regularization.py` module, and a utility function to load the CIFAR-10 dataset. We enable autoreload, so that the imported functions are automatically updated whenever the code is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3f4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_CIFAR_10\n",
    "from loss import SVM_loss, cross_entropy_loss\n",
    "from regularization import L1_reg, L2_reg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c38dc1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "---\n",
    "\n",
    "### 1 Linear Classifiers\n",
    "\n",
    "---\n",
    "\n",
    "Having implemented loss functions and regularizers, we now want to implement linear classifiers for the CIFAR-10 dataset that we already used in the previous problem set. Let's start by loading and inspecting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6acd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50000, 32, 32, 3)\n",
      "Training labels shape: (50000,)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_CIFAR_10('datasets/cifar-10-batches-py')\n",
    "\n",
    "print(\n",
    "    f'Training data shape: {X_train.shape}',\n",
    "    f'Training labels shape: {y_train.shape}',\n",
    "    f'Test data shape: {X_test.shape}',\n",
    "    f'Test labels shape: {y_test.shape}',\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed159f7f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.1 Preprocessing (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Before we start implementing and training our classifiers, we have to perform some preprocessing steps on the dataset.\n",
    "\n",
    "First of all, we want to use some of the images from the training set as our validation set, which we will use to determine the hyperparameters of our models. In addition, we're going to subsample the test set and randomly choose a small training set for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238a03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1000 images for the validation set.\n",
    "indices = np.arange(49000, 50000)\n",
    "X_val = X_train[indices]\n",
    "y_val = y_train[indices]\n",
    "\n",
    "# Use 49000 images for the training set.\n",
    "indices = np.arange(49000)\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Randomly sample 500 training images for the development set.\n",
    "indices = np.random.choice(49000, 500, replace=False)\n",
    "X_dev = X_train[indices]\n",
    "y_dev = y_train[indices]\n",
    "\n",
    "# Use 1000 images for the test set.\n",
    "indices = np.arange(1000)\n",
    "X_test = X_test[indices]\n",
    "y_test = y_test[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ec1d9",
   "metadata": {},
   "source": [
    "The linear classifier model expects each input image to be a vector. Hence we transform the images in all four datasets to vectors. So, if there are $N$ images in the respective set, the shape after the transformation will be $(N, D)$ where $D$ is height times width times channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fddb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all image tensors into vectors.\n",
    "X_train, X_val, X_dev, X_test = [\n",
    "    np.reshape(X, (X.shape[0], -1)) for X in (X_train, X_val, X_dev, X_test)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392fe55c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.1 Normalization (2 Points)\n",
    "\n",
    "The next step is to normalize the data. We want to center the data in the origin, so the task is to subtract the mean from each image in the four datasets. Compute the mean from the *training set* and use it to normalize all the datasets.\n",
    "\n",
    "Write your solution in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28428875",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "###                  START OF YOUR CODE                  ###\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "###                   END OF YOUR CODE                   ###\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf9ca2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.1.3 Implicit Bias (3 Points)\n",
    "\n",
    "Lastly, we want to simplify the score function of the linear classifier by making the bias implicit. That is, we later want to extend the weight matrix with the bias, so that the prediction of the scores becomes a single matrix multiplication.\n",
    "\n",
    "To make that work, add a column of ones to each of the four datasets, such that the dimension of the images is $D+1$, with the value $1$ in the *last* dimension of each image. You may loop over the four datasets as we did when turning the image tensors into vectors above, but otherwise use only vectorized operations.\n",
    "\n",
    "Write your solution in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb564d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "###                  START OF YOUR CODE                  ###\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "###                   END OF YOUR CODE                   ###\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2e1c5",
   "metadata": {},
   "source": [
    "Let's make a sanity check and print out the shapes of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33769411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the shapes.\n",
    "print(\n",
    "    f'Training set shape: {X_train.shape}',\n",
    "    f'Validation set shape: {X_val.shape}',\n",
    "    f'Development set shape: {X_dev.shape}',\n",
    "    f'Test set shape: {X_test.shape}',\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b11263",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.2 Implementation (20 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Now we're ready to implement a linear classifier.\n",
    "\n",
    "Since our data is stored in design matrices with the shape $(N, D+1)$, we define our parameter matrix to have the shape $(D+1, K)$, so that we can compute the scores by\n",
    "\n",
    "$$\n",
    "    S = f(X; W) = XW\n",
    "$$\n",
    "\n",
    "with S having the shape $(N, K)$, as expected by our loss functions.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.2.1 Task\n",
    "\n",
    "Complete the definitions of the `train` and `predict` methods of the `LinearClassifier` class, using only vectorized operations.\n",
    "\n",
    "For the `train` method, implement the following steps for stochastic gradient descent:\n",
    "\n",
    "- Sample randomly and with replacement a mini-batch of input images with corresponding labels from the dataset.\n",
    "- Compute the scores for the inputs.\n",
    "- Compute the complete loss and its gradient with respect to the parameters.\n",
    "- Store the complete loss in the given list for the loss history.\n",
    "- Update the model parameters with the gradient of the complete loss multiplied by the learning rate.\n",
    "\n",
    "<br>\n",
    "\n",
    "Remember that in order to obtain the gradient of the data loss with respect to the parameters, you have to multiply the gradient computed with respect to the scores with the samples of the current mini-batch, using the chain rule for derivatives.\n",
    "\n",
    "For the `predict` method, implement the following steps:\n",
    "\n",
    "- Compute the scores for the inputs.\n",
    "- Return for each sample in the dataset the label for the class with the hightest score.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.2.2 Solution\n",
    "\n",
    "Write your code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier:\n",
    "\n",
    "    def __init__(self, K, L, R):\n",
    "        \"\"\"\n",
    "        Create a linear classifier for the given number of classes.\n",
    "\n",
    "        Parameters:\n",
    "            - K: Number of classes.\n",
    "            - L: Data loss.\n",
    "            - R: Regularization loss.\n",
    "\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.L = L\n",
    "        self.R = R\n",
    "        self.W = None\n",
    "\n",
    "\n",
    "    def train(self, X, y, lr=1e-3, reg=1e-5, num_iters=100, batch_size=128, verbose=False, print_every=100):\n",
    "        \"\"\"\n",
    "        Train the linear classifier using stochastic gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "            - X: Training images with shape (N, D+1) with number of samples N and data dimension D.\n",
    "            - y: Training labels for N images.\n",
    "            - lr: Learning rate.\n",
    "            - reg: Regularization strength.\n",
    "            - num_iters: Number of training iterations.\n",
    "            - batch_size: Number of samples per minibatch.\n",
    "            - verbose: Show loss during training.\n",
    "            - print_every: Interval for showing loss.\n",
    "\n",
    "        Returns:\n",
    "            - loss_history: List with loss for each iteration.\n",
    "\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "\n",
    "        if self.W is None:\n",
    "            \n",
    "            # Transpose the parameter matrix to left multiply the dataset.\n",
    "            self.W = 1e-3 * np.random.randn(D, self.K)\n",
    "\n",
    "        loss_history = []\n",
    "\n",
    "        for i in range(num_iters):\n",
    "            ############################################################\n",
    "            ###                  START OF YOUR CODE                  ###\n",
    "            ############################################################\n",
    "\n",
    "\n",
    "\n",
    "            ############################################################\n",
    "            ###                   END OF YOUR CODE                   ###\n",
    "            ############################################################\n",
    "            if verbose and i % print_every == 0:\n",
    "                print(f'Iteration: {i}/{num_iters} Loss: {loss:.5f}')\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels for a set of test images.\n",
    "\n",
    "        Parameters:\n",
    "            - X: Test images with shape (N, D+1) with number of samples N and data dimension D.\n",
    "\n",
    "        Returns:\n",
    "            - y_pred: Vector of length N with predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        ###                  START OF YOUR CODE                  ###\n",
    "        ############################################################\n",
    "\n",
    "        y_pred = None\n",
    "\n",
    "        ############################################################\n",
    "        ###                   END OF YOUR CODE                   ###\n",
    "        ############################################################\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b262af",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.2.3 Results\n",
    "\n",
    "Let's train the model with the cross-entropy loss and L2 regularization and evaluate it both on the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd827b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "model = LinearClassifier(10, cross_entropy_loss, L2_reg)\n",
    "\n",
    "# Train the model for 1000 iterations.\n",
    "loss_history = model.train(X_train, y_train, num_iters=1000)\n",
    "\n",
    "# Compute accuracy for the training set.\n",
    "y_train_pred = model.predict(X_train)\n",
    "acc_train = np.mean(y_train == y_train_pred)\n",
    "\n",
    "# Compute accuracy for the validation set.\n",
    "y_val_pred = model.predict(X_val)\n",
    "acc_val = np.mean(y_val == y_val_pred)\n",
    "\n",
    "print(f'Train accuracy: {acc_train*100:.2f}%  |  Validation accuracy: {acc_val*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b21c2",
   "metadata": {},
   "source": [
    "We repeat the steps for the multiclass SVM loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "model = LinearClassifier(10, SVM_loss, L2_reg)\n",
    "\n",
    "# Train the model for 1000 iterations.\n",
    "loss_history = model.train(X_train, y_train, num_iters=1000)\n",
    "\n",
    "# Compute accuracy for the training set.\n",
    "y_train_pred = model.predict(X_train)\n",
    "acc_train = np.mean(y_train == y_train_pred)\n",
    "\n",
    "# Compute accuracy for the validation set.\n",
    "y_val_pred = model.predict(X_val)\n",
    "acc_val = np.mean(y_val == y_val_pred)\n",
    "\n",
    "print(f'Train accuracy: {acc_train*100:.2f}%  |  Validation accuracy: {acc_val*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcc87e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.3 Hyperparameter Search (10 Points)\n",
    "\n",
    "---\n",
    "\n",
    "In order to find out which values for the *learning rate* and the *regularization strength* work good for our data, we want to perform a hyperparameter search.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.3.1 Task\n",
    "\n",
    "Choose one of the loss functions and tune the hyperparameters of the model on the validation set, using L2 regularization.\n",
    "\n",
    "Experiment with different ranges for the learning rate and the regularization strength. For each combination compute the accuracy on the validation set. Store the accuracies in the `results` array, such that the indices $(i, j)$ correspond to the $i$-th learning rate and $j$-th regularization strength from the candidate lists which are defined below. Sort the candidate values in ascending order for an interpretable visualization.\n",
    "\n",
    "In order to save computation time, start with a coarse selection of four to five values for each hyperparameter. You can check the results with the code given below to refine the search.\n",
    "\n",
    "You should be able to get $> 35\\%$ accuracy on the test set.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1.3.2 Solution\n",
    "\n",
    "Write your solution in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81848194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define candidate values in ascending order.\n",
    "lrs = []\n",
    "regs = []\n",
    "\n",
    "# Create array to store results.\n",
    "results = np.empty((len(lrs), len(regs)))\n",
    "\n",
    "############################################################\n",
    "###                  START OF YOUR CODE                  ###\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "###                   END OF YOUR CODE                   ###\n",
    "############################################################\n",
    "\n",
    "# Get best accuracy.\n",
    "best_acc = np.max(results)\n",
    "\n",
    "# Get indices of the best values.\n",
    "i, j = np.unravel_index(np.argmax(results), results.shape)\n",
    "\n",
    "# Save the best values.\n",
    "best_lr = lrs[i]\n",
    "best_reg = regs[j]\n",
    "\n",
    "# Show result.\n",
    "print(f'Best validation accuracy: {best_acc*100:.2f}% with lr = {best_lr} and reg = {best_reg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f714e1b",
   "metadata": {},
   "source": [
    "Let's visualize the results of the grid search to get an idea which values worked better or worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "# Set title and axis labels.\n",
    "plt.title('Learning Rate and Regularization Strength')\n",
    "plt.xlabel('reg')\n",
    "plt.ylabel('lr')\n",
    "\n",
    "# Reverse order of entries.\n",
    "res = np.flip(results, axis=0)\n",
    "\n",
    "# Set axis ticks.\n",
    "plt.xticks(ticks=np.arange(len(regs)), labels=regs)\n",
    "plt.yticks(ticks=np.arange(len(lrs)), labels=list(reversed(lrs)))\n",
    "\n",
    "# Show accuracies as heat map.\n",
    "plt.colorbar(plt.imshow(res, cmap='bone', interpolation='bicubic'), ax=plt.gca(), fraction=0.046, pad=0.04)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc2c68",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.3.3 Result\n",
    "\n",
    "Now let's evaluate the best model on the test set. Change the loss function if you worked with the SVM loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5326a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "model = LinearClassifier(10, cross_entropy_loss, L2_reg)\n",
    "\n",
    "# Train the model for some more iterations.\n",
    "loss_history = model.train(X_train, y_train, best_lr, best_reg, num_iters=2_000)\n",
    "\n",
    "# Compute accuracy for the training set.\n",
    "y_train_pred = model.predict(X_train)\n",
    "acc_train = np.mean(y_train == y_train_pred)\n",
    "\n",
    "# Compute accuracy for the test set.\n",
    "y_test_pred = model.predict(X_test)\n",
    "acc_test = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(f'Train accuracy: {acc_train*100:.2f}%  |  Test accuracy: {acc_test*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63013a0",
   "metadata": {},
   "source": [
    "Finaly make a plot with the losses computed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "# Set title and axis labels.\n",
    "plt.title('Loss during Training')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot the loss.\n",
    "plt.plot(np.array(loss_history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c55d5d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.4 Limitations (5 Points)\n",
    "\n",
    "---\n",
    "\n",
    "Give a concise description of the limitations of the linear classifer for image classification. What are the shortcomings of this model preventing it from attaining a high accuracy on natural images? Take into account the different interpretations of the role of the model's parameters.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c5e40",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87846b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "Parts of this exercise are adapted from the Stanford CS231n course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfa41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
